{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhVi_jp9DOpL"
      },
      "source": [
        "# MPI (Message-Passing Interface)\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "* Mechanism of data exchange between processes\n",
        "* Two types of communication:\n",
        " * **point-to-point**: between two processes\n",
        " * **collective**: between multiple processes\n",
        "* Typically only one program, branching depending on the process\n",
        "* Using the mpi4py Python library\n",
        "\n",
        "An mpi4py tutorial:\n",
        "* https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
        "\n",
        "\n",
        "Install mpi4py:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgyeyISlEBPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ef9b94-7cae-4d95-84e5-a24481a19b5b"
      },
      "source": [
        "!pip install mpi4py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m1.8/2.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp310-cp310-linux_x86_64.whl size=2744874 sha256=b24f0771ff64bb1318a837bc1e7b788e1f06ace1ccd466448de5d51f66e9d53f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/1b/b5/97ec4cfccdde26e0f3590ad6e09a5242d508dff09704ef86c1\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89pnn-HgDpjV"
      },
      "source": [
        "## A basic example (no data exchange)\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk1x92wcDOpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa305613-dae1-405f-cd48-087b2be87a24"
      },
      "source": [
        "# mpi.py\n",
        "from mpi4py import MPI\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank() # index of the current process\n",
        "print (\"hello from process \", rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello from process  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 2 python mpi.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRqPHFyAv-4I",
        "outputId": "323cd809-6a72-49a3-91b2-3aa746d22374"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello from process  0\n",
            "hello from process  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UptBtCQoDOqK"
      },
      "source": [
        "## Point-to-point communication of two processes\n",
        "\n",
        "### Example: computing $\\pi$ with MPI\n",
        "\n",
        "$$\\pi=\\sqrt{6\\sum_{n=1}^{\\infty}\\frac{1}{n^2}}$$\n",
        "\n",
        "**Exercise:** Theoretically estimate the error resulting if we truncate the series at $N$ terms.  \n",
        "\n",
        "Without MPI:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJeSQbxbDOqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eaba809-e066-4492-8b9c-1942d0488434"
      },
      "source": [
        "import numpy as np\n",
        "a = np.arange(1,200000)\n",
        "print (np.sqrt(6*np.sum(1./(a*a))))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1415878789259364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrFHlcHIDOqz"
      },
      "source": [
        "### Functions ``send()``, ``recv()``\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 2 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NFhjMInDOq4"
      },
      "source": [
        "# Evaluate the sum of 2M terms by splitting into two groups of M terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "print ('Process', rank, 'found partial sum from term', 1+rank*M, 'to term', 1+(rank+1)*M-1, ': ', s )\n",
        "\n",
        "# process 1 sends its partial sum to process 0\n",
        "if rank == 1:\n",
        "    comm.send(s, dest=0)\n",
        "\n",
        "# process 0 receives the partial sum from process 1, adds to its own partial sum\n",
        "# and outputs the result\n",
        "elif rank == 0:\n",
        "    s_other = comm.recv(source=1)\n",
        "    s_total = s+s_other\n",
        "    print ('total partial sum =', s_total)\n",
        "    print ('pi_approx =', np.sqrt(6*s_total))\n",
        "\n",
        "print ('Process', rank, 'finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaN4fOCFnfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa490109-f7a4-44ce-a878-3b333e4ddf3b"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 2 python mpi.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 1 found partial sum from term 101 to term 200 :  0.004962645830104402\n",
            "Process 1 finished\n",
            "Process 0 found partial sum from term 1 to term 100 :  1.6349839001848931\n",
            "total partial sum = 1.6399465460149976\n",
            "pi_approx = 3.1368263063309683\n",
            "Process 0 finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-wSXWdXH1pY"
      },
      "source": [
        "**Exercise:** Perform the same computation in a \"ping-pong\" manner: one process sums only even terms, the other only odd terms; after adding a new term the process sends the current result to the other process.  \n",
        "\n",
        "## Collective communication\n",
        "\n",
        "Perform efficient (fast, load-balanced) collective operations (e.g., summations) involving multiple processes.\n",
        "\n",
        "<img src='https://materials.jeremybejarano.com/MPIwithPython/_images/fastSum.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL5OC2CNDOrY"
      },
      "source": [
        "#from IPython.display import Image\n",
        "#Image(filename=\"fastSum.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5eg3eZdDOr1"
      },
      "source": [
        "### Function ``gather()``\n",
        "\n",
        "Pass data from all processes to the chosen process.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 5 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGNWAOcPDOr-"
      },
      "source": [
        "# Evaluate the sum of MN terms by splitting into M groups of N terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # total number of processes\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "\n",
        "partialSums = comm.gather(s, root=0)\n",
        "print ('partialSums gathered at process %d:' %(rank), partialSums)\n",
        "\n",
        "if rank == 0:\n",
        "    print ('pi_approx =', np.sqrt(6*np.sum(partialSums)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWUfMyhSGG6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a91257-bfbc-4652-d738-be1a2eadd91a"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python mpi.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "partialSums gathered at process 4: None\n",
            "partialSums gathered at process 1: None\n",
            "partialSums gathered at process 3: None\n",
            "partialSums gathered at process 2: None\n",
            "partialSums gathered at process 0: [1.6349839001848931, 0.004962645830104402, 0.0016597368826256017, 0.0008309063464401552, 0.0004988762708311448]\n",
            "pi_approx = 3.1396841231387222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCXH5iJXICPd"
      },
      "source": [
        "### Function ``bcast()``\n",
        "\n",
        "Pass data from the chosen process to all other processes.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw2GT71eDOsY"
      },
      "source": [
        "# basic usage of bcast()\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "if rank == 0:\n",
        "    some_data = {0: 'abcd', 1:1234}\n",
        "else:\n",
        "    some_data = None\n",
        "\n",
        "print (\"I'm process\", rank, '; data before broadcasting:', some_data)\n",
        "data = comm.bcast(some_data, root=0)\n",
        "print (\"I'm process\", rank, '; data after broadcasting:', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRlOFLVFGL-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc79eea1-27d6-49c2-f751-7b8bb42f6d2b"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 3 python mpi.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm process 1 ; data before broadcasting: None\n",
            "I'm process 2 ; data before broadcasting: None\n",
            "I'm process 0 ; data before broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 0 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 1 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 2 ; data after broadcasting: {0: 'abcd', 1: 1234}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNm6buMYITTW"
      },
      "source": [
        "### Functions ``scatter()``, ``reduce()``\n",
        "\n",
        "* ``scatter()``: distribute data from one source to all processes\n",
        "* ``reduce()``: combine data from all processes using a collective operation like `sum` or `max`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTGFw1YLDOsq"
      },
      "source": [
        "# Evaluate the sum of N terms by scattering them to N processes.\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "if rank == 0:\n",
        "    data2scatter = [a*a for a in range(1,size+1)]\n",
        "else:\n",
        "    data2scatter = None\n",
        "\n",
        "data = comm.scatter(data2scatter, root=0)\n",
        "\n",
        "print ('Data at process', rank, ':', data)\n",
        "\n",
        "b = 1./data\n",
        "\n",
        "partialSum = comm.reduce(b, op = MPI.SUM, root = 0)\n",
        "\n",
        "print ('Partial sum at process', rank, ':', partialSum)\n",
        "\n",
        "if rank == 0:\n",
        "    result = np.sqrt(6*partialSum)\n",
        "    print ('Pi_approx:', result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5NWqw3HGRTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e89336-be45-4e15-b0fe-d8bec6b37950"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python mpi.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data at process 0 : 1\n",
            "Data at process 2 : 9\n",
            "Data at process 3 : 16\n",
            "Data at process 1 : 4\n",
            "Data at process 4 : 25\n",
            "Partial sum at process 3 : None\n",
            "Partial sum at process 4 : None\n",
            "Partial sum at process 2 : None\n",
            "Partial sum at process 1 : None\n",
            "Partial sum at process 0 : 1.4636111111111112\n",
            "Pi_approx: 2.9633877010385707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tmm87fhIdwU"
      },
      "source": [
        "## Example: parallel scalar product\n",
        "* Generate two random vectors $\\mathbf x$ and $\\mathbf y$ at the root process. Goal: compute their scalar product $\\langle\\mathbf x,\\mathbf y\\rangle$\n",
        "* Divide $\\mathbf x$ and $\\mathbf y$ into chunks and scatter them to the other processes\n",
        "* Compute scalar products between chunks at each process\n",
        "* Obtain $\\langle\\mathbf x,\\mathbf y\\rangle$ by reducing (summing) local scalar products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiixH1tCDOs9"
      },
      "source": [
        "#\"to run\" syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #length of vectors\n",
        "\n",
        "#arbitrary example vectors, generated to be evenly divided by the number of\n",
        "#processes for convenience\n",
        "\n",
        "x = np.random.rand(N) if comm.rank == 0 else None\n",
        "y = np.random.rand(N) if comm.rank == 0 else None\n",
        "\n",
        "#initialize as numpy arrays\n",
        "dot = np.array([0.])\n",
        "local_N = np.array([0])\n",
        "\n",
        "#test for conformability\n",
        "if rank == 0:\n",
        "                if (N != y.size):\n",
        "                                print (\"vector length mismatch\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #currently, our program cannot handle sizes that are not evenly divided by\n",
        "                #the number of processors\n",
        "                if (N % size != 0):\n",
        "                                print (\"the number of processors must evenly divide n.\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #length of each process's portion of the original vector\n",
        "                local_N = np.array([N//size])\n",
        "\n",
        "#communicate local array size to all processes\n",
        "comm.Bcast(local_N, root=0)\n",
        "\n",
        "#initialize as numpy arrays\n",
        "local_x = np.zeros(local_N)\n",
        "local_y = np.zeros(local_N)\n",
        "\n",
        "#divide up vectors\n",
        "comm.Scatter(x, local_x, root=0)\n",
        "comm.Scatter(y, local_y, root=0)\n",
        "\n",
        "#local computation of dot product\n",
        "local_dot = np.array([np.dot(local_x, local_y)])\n",
        "\n",
        "#sum the results of each\n",
        "comm.Reduce(local_dot, dot, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "                print (\"The dot product computed with MPI:\", dot[0])\n",
        "                print (\"The dot product computed w/o  MPI:\", np.dot(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjotqyGTHGfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d87c184-bf63-4e27-f2ab-9ad806503332"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 10 python mpi.py 4000000"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product computed with MPI: 999982.3558001126\n",
            "The dot product computed w/o  MPI: 999982.3558001126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcjQGyquDOtQ"
      },
      "source": [
        "\n",
        "**Exercise:** Why is the result $\\approx$ 1E6? A bad RNG?\n",
        "\n",
        "### Reduce-based computation of $\\pi$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODozaK9rDOtS"
      },
      "source": [
        "# run syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #number of terms\n",
        "\n",
        "#initialize as numpy array\n",
        "s = np.array([0.])\n",
        "\n",
        "#test for conformability\n",
        "if (N % size != 0):\n",
        "    print (\"the number of processors must evenly divide n.\")\n",
        "    comm.Abort()\n",
        "\n",
        "#length of each process's portion of the original vector\n",
        "local_N = np.array([N/size])\n",
        "\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "#local computation of partial sum\n",
        "local_s = getPartialSum(1+rank*local_N, 1+(rank+1)*local_N)\n",
        "local_s = np.array([local_s])\n",
        "\n",
        "#sum the results of each local sum\n",
        "comm.Reduce(local_s, s, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "    pi_approx = np.sqrt(6*s[0])\n",
        "    print (\"pi_approx:\", pi_approx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOuSPxKJDOtl"
      },
      "source": [
        "The program execution time can be measured with commands `time` or `/usr/bin/time -v`:\n",
        "\n",
        "(`real`: wall clock time).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIc3lRQwJFjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c449874-4c5a-4d8a-94bf-4664faac031d"
      },
      "source": [
        "!time mpirun --allow-run-as-root --oversubscribe -n 1 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root --oversubscribe -n 2 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root --oversubscribe -n 4 python mpi.py 100000000"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pi_approx: 3.14159264404049\n",
            "\n",
            "real\t0m1.389s\n",
            "user\t0m0.616s\n",
            "sys\t0m0.640s\n",
            "pi_approx: 3.1415926440404958\n",
            "\n",
            "real\t0m1.594s\n",
            "user\t0m1.376s\n",
            "sys\t0m1.097s\n",
            "pi_approx: 3.1415926440404975\n",
            "\n",
            "real\t0m2.761s\n",
            "user\t0m1.966s\n",
            "sys\t0m1.478s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLSL8ssHKvq3"
      },
      "source": [
        "Speedup and efficiency of parallelization with 2 processes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbMya2FYDOto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9bc8a5-3278-49d3-d911-31cab7aff5cc"
      },
      "source": [
        "Speedup = 1.389/1.594\n",
        "print ('Speedup:', Speedup)\n",
        "\n",
        "Efficiency = Speedup/2\n",
        "print ('Efficiency:', Efficiency)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup: 0.8713927227101631\n",
            "Efficiency: 0.43569636135508155\n"
          ]
        }
      ]
    }
  ]
}