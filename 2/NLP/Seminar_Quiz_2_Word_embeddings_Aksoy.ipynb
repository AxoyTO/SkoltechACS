{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task: word sense in context\n",
        "\n",
        "In this task you will perform embeddings of ambigous words and find least similar context which normally shall corespond to words with different meanings (e.g. java language vs java island). You will use similarity search between word embeddings to reach this goal.  \n",
        "\n",
        "How to proceed:\n",
        "\n",
        "- Load text from wikitext dataset as shown below\n",
        "\n",
        "- Write code corresponding to instructions below inside comments\n",
        "\n"
      ],
      "metadata": {
        "id": "gz5oMGpClMos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "EWFUelVKTfg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b063178-527d-4c89-e04e-d8c8c3d42973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.5)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIRV64r1PQRE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hitq6P0iRnkK",
        "outputId": "245414bd-1b2d-42c1-dc6f-85641d763f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 4358\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 36718\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 3760\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "import spacy\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "target_words = [\"python\", \"jaguar\", \"apple\", \"bank\", \"java\"]\n",
        "stopwords = set([\"a\", \"an\", \"the\", \"is\", \"are\", \"in\", \"on\", \"at\"])\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "j = 0\n",
        "contexts = {}\n",
        "word_counts = {word: 0 for word in target_words}\n",
        "\n",
        "for i, t in enumerate(dataset[\"train\"]):\n",
        "  line = t[\"text\"]\n",
        "  entities = []\n",
        "  doc = nlp(line)\n",
        "  for ent in doc.ents:\n",
        "    if ent.text.lower() in target_words:\n",
        "      entities.append(ent.text.lower())\n",
        "\n",
        "  if entities:\n",
        "    for entity in entities:\n",
        "      with open(f\"{entity}_entities.txt\", \"a\") as file:\n",
        "        file.write(line + \"\\n\")\n",
        "\n",
        "  words = line.lower().split()\n",
        "  search = intersection(target_words, words)\n",
        "\n",
        "  for s in search:\n",
        "    print(s, \">>>\", line)\n",
        "    j += 1\n",
        "\n",
        "  if j > 20:\n",
        "    break\n",
        "\n",
        "\n",
        "  context = [w for w in words if w not in target_words and w not in stopwords]\n",
        "  context_length = len(context)\n",
        "\n",
        "  if 5 <= context_length <= 20:\n",
        "    for s in search:\n",
        "      if word_counts[s] < 200:\n",
        "        if s not in contexts:\n",
        "          contexts[s] = []\n",
        "\n",
        "        contexts[s].append(context)\n",
        "        word_counts[s] += 1\n",
        "\n",
        "  if all(count >= 200 for count in word_counts.values()):\n",
        "    break\n",
        "\n",
        "context_vectors = {}\n",
        "for word, contexts_list in contexts.items():\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  context_matrix = vectorizer.fit_transform([\" \".join(context) for context in contexts_list])\n",
        "  context_sum = np.sum(context_matrix.toarray(), axis=1)\n",
        "  context_vectors[word] = context_sum\n",
        "\n",
        "least_similar = {}\n",
        "for word, vectors in context_vectors.items():\n",
        "  distances = cosine_distances(vectors.reshape(1, -1), vectors)\n",
        "  least_similar_indices = np.argsort(distances)[0, 1:11]\n",
        "  least_similar[word] = [contexts_list[i] for i in least_similar_indices]\n",
        "\n",
        "\n",
        "for word, contexts_list in least_similar.items():\n",
        "  print(f\"{word}:\")\n",
        "  for i, context in enumerate(contexts_list):\n",
        "    print(f\"Context {i+1}: {context}\")\n",
        "  print()\n",
        "\n",
        "# 1) Find lines which contain target words (from target_words): words with potentially multiple meanings\n",
        "# 2) Exclude mentions of all target word from the found line (context) and stopwords (make sure the length of the remaining text contain at least 5 words but not more than 20 words)\n",
        "# 3) Vectorize context by summing up all word embeddings from this context (one line shall correspond to one vector)\n",
        "# You are free to use any vectorization method studied at the lecture / seminar\n",
        "# 4) Save into a data structure of your choice pairs \"word , context_vector\"\n",
        "# 5) Limit the number of occurrences to 200 per word.\n",
        "# 6) For each word print top 10 pairs of LEAST similar vectors e.g. \"java | context_1 | context_2\" in a form of a table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC-PKmLvRpuq",
        "outputId": "ce86381f-dade-461a-c87e-9b5621d08371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bank >>>  Fingal was designed and built as a <unk> by J & G Thomson 's Clyde Bank Iron Shipyard at <unk> in Glasgow , Scotland , and was completed early in 1861 . She was described by <unk> <unk> Scales , who served on the Atlanta before her battle with the monitors , as being a two @-@ <unk> , iron @-@ <unk> ship 189 feet ( 57 @.@ 6 m ) long with a beam of 25 feet ( 7 @.@ 6 m ) . She had a draft of 12 feet ( 3 @.@ 7 m ) and a depth of hold of 15 feet ( 4 @.@ 6 m ) . He estimated her tonnage at around 700 tons <unk> . Fingal was equipped with two vertical single @-@ cylinder direct @-@ acting steam engines using steam generated by one <unk> @-@ tubular boiler . The engines drove the ship at a top speed of around 13 knots ( 24 km / h ; 15 mph ) . They had a bore of 39 inches ( 991 mm ) and a stroke of 30 inches ( 762 mm ) . \n",
            "\n",
            "bank >>>  Contemporary reviews of the Type 1 design were generally favorable . The New York Weekly Tribune on May 19 , 1849 described the new dollar as \" undoubtedly the <unk> , <unk> , lightest , coin in this country ... it is too delicate and beautiful to pay out for potatoes , and <unk> , and salt pork . Oberon might have paid Puck with it for bringing the blossom which bewitched <unk> . \" Willis ' Bank Note List stated that \" there is no probability of them ever getting into general circulation ; they are altogether too small . \" The North Carolina Standard hoped that they would be struck at the Charlotte Mint and circulated locally to eliminate the problem of small @-@ denomination bank notes from out of state . <unk> dealer and numismatic author Q. David Bowers notes that the head of Liberty on the Type 1 dollar is a scaled @-@ down version of that on the double eagle , and \" a nicely preserved gold dollar is beautiful to <unk> \" . \n",
            "\n",
            "bank >>>  A total of 19 @,@ 499 @,@ 337 gold dollars were coined , of which 18 @,@ 223 @,@ <unk> were struck at Philadelphia , 1 @,@ <unk> @,@ 000 at New Orleans , 109 @,@ 138 at Charlotte , 90 @,@ 232 at San Francisco and 72 @,@ 529 at Dahlonega . According to an advertisement in the February 1899 issue of The <unk> , gold dollars brought $ 1 @.@ 80 each , still in demand as a birthday present and for jewelry . That journal in 1905 carried news of a customer depositing 100 gold dollars into a bank ; the teller , aware of the value , credited the account with $ 1 @.@ 60 per coin . In 1908 , a dealer offered $ 2 each for any quantity . As coin collecting became a widespread <unk> in the early 20th century , gold dollars became a popular specialty , a status they retain . The 2014 edition of <unk> <unk> 's A Guide Book of United States Coins rates the least expensive gold dollar in very fine condition ( <unk> @-@ 20 ) at $ 300 , a value given for each of the Type 1 Philadelphia issues from 1849 to 1853 . Those seeking one of each type will find the most expensive to be a specimen of the Type 2 , with the 1854 and 1855 estimated at $ 350 in that condition ; the other two types have dates valued at $ 300 in that grade . \n",
            "\n",
            "apple >>>  On 15 November 2010 , the hosts of the Fitwatch blog were asked by the Police National E @-@ Crime Unit to take down the website due to it \" being used to undertake criminal activities \" . The request came after a post on the blog after the 2010 student protest in London , which advised students of actions they should take if they were concerned that they were photographed at the demonstration , such as cutting their hair and <unk> of clothing they were wearing . Emily Apple , one of the founders of the site told The Guardian , \" Nothing in that post [ giving guidance to student protesters ] has not been said before on our blog or on other sites \" . On 17 November 2010 , the Fitwatch website returned , hosted on a web server outside of the UK . \n",
            "\n",
            "bank >>>  Following her commissioning on 1 October 1914 , Markgraf conducted sea trials , which lasted until 12 December . By 10 January 1915 , the ship had joined III Battle Squadron of the High Seas Fleet with her three sister ships . On 22 January 1915 , III Squadron was detached from the fleet to conduct maneuver , gunnery , and torpedo training in the Baltic . The ships returned to the North Sea on 11 February , too late to assist the I Scouting Group at the Battle of Dogger Bank . \n",
            "\n",
            "bank >>>  In the aftermath of the loss of SMS <unk> at the Battle of Dogger Bank , Kaiser Wilhelm II removed Admiral Friedrich von <unk> from his post as fleet commander on 2 February . Admiral Hugo von Pohl replaced him as commander of the fleet ; von Pohl carried out a series of sorties with the High Seas Fleet throughout 1915 . The first such operation — Markgraf 's first with the fleet — was a fleet advance to <unk> on 29 – 30 March ; the German fleet failed to engage any British warships during the sortie . Another uneventful operation followed on 17 – 18 April , and another three days later on 21 – 22 April . Markgraf and the rest of the fleet remained in port until 29 May , when the fleet conducted another two @-@ day advance into the North Sea . On 11 – 12 September , Markgraf and the rest of III Squadron supported a <unk> operation off <unk> . Another uneventful fleet advance followed on 23 – 24 October . \n",
            "\n",
            "bank >>>  The operation began on 12 October , when Moltke and the four König @-@ class ships covered the landing of ground troops by suppressing the shore batteries covering <unk> Bay . Markgraf fired on the battery located on Cape <unk> . After the successful amphibious assault , III Squadron steamed to <unk> <unk> , although Markgraf remained behind for several days . On the 17th , Markgraf left <unk> Bay to rejoin her squadron in the Gulf of Riga , but early on the following morning she ran aground at the entrance to <unk> . The ship was quickly freed , and she reached the III Squadron anchorage north of <unk> Bank on the 19th . The next day , Markgraf steamed to Moon Sound , and on the 25th participated in the bombardment of Russian positions on the island of <unk> . The ship returned to <unk> on 27 October , and two days later was detached from Operation Albion to return to the North Sea . \n",
            "\n",
            "bank >>>  Then the video shows various shots at Waterloo station , as the chorus starts . In slow motion , the camera pans across the <unk> shop on the station <unk> as the duo walk past . It cuts to a brief shot of a No. 42 red double @-@ <unk> bus , showing the destination as <unk> , also advertising the stage @-@ show Evita , then black and white shots of the Tower Bridge , Westminster and the Westminster Palace <unk> Tower from the sky . The duo poses on the South Bank of the River Thames in a pastiche of a <unk> image , with the Houses of Parliament as a background . \n",
            "\n",
            "apple >>>  Anderson was a standby director during the 2005 filming of Robert Altman 's A Prairie Home Companion for insurance purposes , as Altman was 80 years old at the time . In addition to films , Anderson has directed several music videos , including several for musician Fiona Apple . In 2008 , Anderson co @-@ wrote and directed a 70 @-@ minute play at the <unk> Theatre , comprising a series of vignettes starring Maya Rudolph and Fred <unk> , with a live musical score by Jon <unk> . \n",
            "\n",
            "apple >>>  Anderson dated ( and frequently collaborated with ) singer Fiona Apple for several years during the late 1990s and early 2000s . He has been in a relationship with actress and comedian Maya Rudolph since 2001 . They live together in the San Fernando Valley with their four children : daughters Pearl Bailey ( born October 2005 ) , Lucille ( born November 2009 ) , and Minnie Ida ( born August 2013 ) and son Jack ( born July 2011 ) . \n",
            "\n",
            "bank >>>  Ross also excelled in baseball , football , <unk> and motorcycle racing . Before he became a hockey executive , he had a career as a bank clerk and ran a sporting @-@ goods store in Montreal . Ross had moved to Brandon , Manitoba , in 1905 at the advice of his parents so he could get a job with a bank , with a salary of $ 600 per year . He gave that career up when he began playing hockey professionally . He was married to <unk> , a native of Montreal , and had two sons , Art and John . During the Second World War , both sons served in the Royal Canadian Air Force . After the war Ross made his son Art the business manager for the Bruins . Ross was named coach and manager of the Boston Bruins in 1924 and moved his family to <unk> , Massachusetts , a suburb of Boston , after being hired . In 1928 , he served as the traveling secretary of the Boston Braves baseball team , which was owned by Bruins owner Charles Adams . He became a naturalized American citizen on April 22 , 1938 . On August 5 , 1964 , Ross died at a nursing home in <unk> , Massachusetts , a suburb of Boston , at the age of 79 . A sister , both his sons , and three grandchildren survived him . \n",
            "\n",
            "jaguar >>>  Ruler 1 is depicted on a couple of Early Classic monuments , the better preserved of which is an altar that dates to <unk> . A ruler known as Jaguar Bird <unk> is represented on a 6th @-@ century stela , which describes him <unk> to the throne in 568 . \n",
            "\n",
            "bank >>>  Ruler 4 was succeeded by K 'inich Ich 'aak Chapat in <unk> . Around 725 Toniná fought a war against Piedras Negras , a city on the north bank of the Usumacinta River , now in Guatemala . A series of events during his reign were marked on monuments between <unk> and 729 and in 730 he <unk> the tomb of his predecessor K 'inich B 'aaknal Chaak . The mother of K 'inich Ich 'aak Chapat is named as Lady <unk> <unk> K 'awiil and his father may well have been K 'inich B 'aaknal Chaak himself . The reign of K 'inich Ich 'aak Chapat is notable for the absence of the usual sculptures depicting bound war captives , although the reason for this is unknown . \n",
            "\n",
            "bank >>>  Though the storm stalled economic development and the city of Houston developed as the region 's principal metropolis , Galveston economic leaders recognized the need to <unk> from the traditional port @-@ related industries . In 1905 William Lewis Moody , Jr. and Isaac H. <unk> , members of two of Galveston 's leading families , founded the American National Insurance Company . Two years later , Moody established the City National Bank , which would later become the Moody National Bank . \n",
            "\n",
            "bank >>>  American National Insurance Company , one of the largest life insurance companies in the United States , is based in Galveston . The company and its <unk> operate in all 50 U.S. states , the District of Columbia , Puerto Rico , and American Samoa . Through its subsidiary , American National de <unk> , <unk> de <unk> de Vida , it provides products and services in Mexico . Moody National Bank , with headquarters in downtown Galveston , is one of the largest privately owned Texas @-@ based banks . Its trust department , established in 1927 , administers over 12 billion dollars in assets , one of the largest in the state . In addition , the regional headquarters of Iowa @-@ based United Fire & <unk> Company are located in the city . \n",
            "\n",
            "bank >>>  In October 2015 , Galveston Arts Center will celebrate relocation to its original home , the historic 1878 First National Bank Building on the Strand . This Italianate @-@ style 1900 Storm survivor was extensively damaged during Hurricane Ike in 2008 . Fortunately , just weeks before Ike made landfall , scaffolding was installed to support the entire structural load of the building for repairs , likely preventing collapse under heavy winds and storm surge . After a lengthy fundraising campaign , restoration is nearing completion . \n",
            "\n",
            "bank >>>  Galveston 's modern architecture include the American National Insurance Company Tower ( One Moody Plaza ) , San Luis Resort South and North Towers , The <unk> <unk> , The <unk> Resort and <unk> , One <unk> Moody Plaza , US National Bank Building , the <unk> Pyramid at Moody Gardens , John Sealy Hospital Towers at <unk> and Medical Arts Building ( also known as Two Moody Plaza ) . \n",
            "\n",
            "bank >>>  Sarnia is a city in Southwestern Ontario , Canada , and had a 2011 population of 72 @,@ 366 . It is the largest city on Lake Huron and in Lambton County . Sarnia is located on the eastern bank of the junction between the Upper and Lower Great Lakes where Lake Huron flows into the St. Clair River , which forms the Canada @-@ United States border , directly across from Port Huron , Michigan . The city 's natural harbour first attracted the French explorer La Salle , who named the site \" The Rapids \" when he had horses and men pull his 45 tonnes ( 50 short tons ; 44 long tons ) barque \" Le <unk> \" up the almost four @-@ knot current of the St. Clair River on 23 August 1679 . \n",
            "\n",
            "bank >>>  Finkelstein received his Master 's degree in political science in 1980 , and later his PhD in political studies , from Princeton . His doctoral thesis was on Zionism . Before gaining academic employment , Finkelstein was a part @-@ time social worker with teenage <unk> in New York . He then taught successively at Rutgers University , New York University , Brooklyn College , and Hunter College and at DePaul University in Chicago . During the First Intifada , he spent every summer from 1988 in the West Bank , a guest of Palestinian families in Hebron and Beit <unk> . \n",
            "\n",
            "bank >>>  Finkelstein was questioned after his arrival at Ben Gurion Airport near Tel Aviv and detained for 24 hours in a holding cell . After speaking to Israeli attorney Michael <unk> he was placed on a flight back to Amsterdam , his point of origin . In an interview with <unk> , Finkelstein stated \" I did my best to provide absolutely <unk> and comprehensive answers to all the questions put to me . I am confident that I have nothing to hide ... no suicide missions or secret rendezvous with terrorist organizations . \" He had been travelling to visit friends in the West Bank and stated he had no interest in visiting Israel . <unk> said banning Finkelstein from entering the country \" recalls the behavior of the Soviet bloc countries \" . \n",
            "\n",
            "bank >>>  Finkelstein is a sharp critic of the state of Israel . Discussing Finkelstein 's book Beyond Chutzpah , Israeli historian Avi Shlaim stated that Finkelstein 's critique of Israel \" is based on an amazing amount of research . He seems to have read everything . He has gone through the reports of Israeli groups , of human rights groups , Human Rights Watch and Peace Now and B <unk> , all of the reports of Amnesty International . And he deploys all this evidence from Israeli and other sources in order to sustain his critique of Israeli practices , Israeli violations of human rights of the Palestinians , Israeli house <unk> , the targeted <unk> of Palestinian militants , the cutting down of trees , the building of the wall — the security barrier on the West Bank , which is illegal — the restrictions imposed on the Palestinians in the West Bank , and so on and so forth . I find his critique extremely detailed , well @-@ documented and accurate . \" \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label found senses (optional for additional points)\n",
        "\n",
        "Add manual labels to 10 rows out of 50 rows in the final table labelling them with hypernyms (e.g. python --> snake or python --> language)\n",
        "\n",
        "Example of the table is presented below\n",
        "\n",
        "| Word         | Context 1     | Context 1 Label| Context 2 | Context 2 Label |\n",
        "|--------------|-----------|------------|---|---|\n",
        "| java | I program with Java      | language | I brew coffe from Java | island |\n",
        "| python      | I seen a python | snake  | I've coded it using python | language   |\n",
        "\n"
      ],
      "metadata": {
        "id": "C89DRaYRljw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table = {\n",
        "   'Word': ['java', 'python', 'bank', 'apple', 'jaguar'],\n",
        "   'Context 1': ['I program with Java', 'I seen a python', 'I went to the bank', 'I ate an apple', 'I saw a jaguar'],\n",
        "   'Context 2': ['I brew coffee from Java island', \"I've coded it using python\", 'I have a bank account', 'I love apple pie', 'I drove a jaguar car'],\n",
        "}\n",
        "\n",
        "labels_dict = {\n",
        "   'python': 'snake',\n",
        "   'bank': 'financial institution',\n",
        "   'apple': 'fruit',\n",
        "   'jaguar': 'animal',\n",
        "}\n",
        "\n",
        "label_list = ['language', 'snake', 'financial institution', 'fruit', 'animal']\n",
        "\n",
        "table['Context 1 Label'] = ['language' if word == 'java' else 'snake' if word == 'python' else '' for word in table['Word']]\n",
        "table['Context 2 Label'] = ['' for _ in range(len(table['Word']))]\n",
        "\n",
        "table['Context 1 Label'] = [labels_dict.get(word, '') for word in table['Word']]\n",
        "table['Context 2 Label'] = [labels_dict.get(word, '') for word in table['Word']]\n",
        "\n",
        "for i in range(len(table['Word'])):\n",
        "   if table['Word'][i] in ['bank', 'apple', 'jaguar']:\n",
        "       table['Context 2 Label'][i] = label_list.pop(0)\n",
        "\n",
        "print(\"Word\\tContext 1\\tContext 1 Label\\tContext 2\\tContext 2 Label\")\n",
        "for i in range(len(table['Word'])):\n",
        "   print(f\"{table['Word'][i]}\\t{table['Context 1'][i]}\\t{table['Context 1 Label'][i]}\\t{table['Context 2'][i]}\\t{table['Context 2 Label'][i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcNd9paMiErY",
        "outputId": "a397d780-e562-467f-cc7d-03267d99b2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word\tContext 1\tContext 1 Label\tContext 2\tContext 2 Label\n",
            "java\tI program with Java\t\tI brew coffee from Java island\t\n",
            "python\tI seen a python\tsnake\tI've coded it using python\tsnake\n",
            "bank\tI went to the bank\tfinancial institution\tI have a bank account\tlanguage\n",
            "apple\tI ate an apple\tfruit\tI love apple pie\tsnake\n",
            "jaguar\tI saw a jaguar\tanimal\tI drove a jaguar car\tfinancial institution\n"
          ]
        }
      ]
    }
  ]
}