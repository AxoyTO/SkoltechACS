{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task: add a new class to CoNLL dataset and find occurrences in wikitext\n",
        "\n",
        "In this task you will retrain a sequence labelling model as done during the seminar, but with one modification. You will add a new class corresponding to nationality like Brazilian, which is usually falls into MISC class of CoNLL. Instead you will try to detect it as a separate class. You are free to use any sequence classification model explored in the seminar.\n",
        "\n",
        "How to proceed:\n",
        "\n",
        "- Load text from wikitext dataset as shown below\n",
        "\n",
        "- Write code corresponding to instructions below inside comments\n",
        "\n"
      ],
      "metadata": {
        "id": "gz5oMGpClMos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install  datasets"
      ],
      "metadata": {
        "id": "EWFUelVKTfg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4acbffe-7611-4bfa-ab58-e7d90775676b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bIRV64r1PQRE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def modify_labels(example):\n",
        "  labels = example[\"ner_tags\"]\n",
        "  modified_labels = []\n",
        "  for label in labels:\n",
        "    if label == \"I-MISC\":\n",
        "      if example[\"tokens\"][label] in nationality_list:\n",
        "        modified_labels.append(\"I-NAT\")\n",
        "      else:\n",
        "        modified_labels.append(label)\n",
        "    else:\n",
        "      modified_labels.append(label)\n",
        "  example[\"ner_tags\"] = modified_labels\n",
        "  return example\n",
        "\n",
        "nationality_regex = re.compile(r\"\\b[A-Z][a-z]+(?:n|ish|ese|ian)\\b\")\n",
        "\n",
        "all_entities = []\n",
        "for i, example in enumerate(dataset[\"train\"]):\n",
        "  text = example[\"text\"]\n",
        "  entities = []\n",
        "  for match in re.finditer(nationality_regex, text):\n",
        "    entity = match.group(0)\n",
        "    entities.append(entity)\n",
        "  all_entities.extend(entities)\n",
        "\n",
        "  with open(f\"entities_{i}.txt\", \"w\") as file:\n",
        "    for entity in entities:\n",
        "      file.write(f\"{entity}\\tNAT\\n\")\n",
        "\n",
        "  if i > 10:\n",
        "    break\n",
        "\n",
        "conll = load_dataset(\"conll2003\")\n",
        "\n",
        "nationality_list = [\"Brazilian\", \"Spanish\", \"American\", \"French\", \"German\", \"Italian\", \"Japanese\", \"Chinese\"]\n",
        "\n",
        "conll[\"train\"] = conll[\"train\"].map(modify_labels)\n",
        "\n",
        "top_entities = Counter(entities).most_common(20)\n",
        "df_entities = pd.DataFrame(top_entities, columns=[\"Entity\", \"Frequency\"])\n",
        "df_entities[\"Entity-Type\"] = \"NAT\"\n",
        "\n",
        "print(\"Entity\\tEntity-Type\\tFrequency\")\n",
        "for _, row in df_entities.iterrows():\n",
        "  print(f\"{row['Entity']}\\t{row['Entity-Type']}\\t{row['Frequency']}\")\n",
        "\n",
        "tag_counts = Counter(all_entities).most_common(20)\n",
        "\n",
        "df_tags = pd.DataFrame(tag_counts, columns=[\"Tag\", \"Frequency\"])\n",
        "\n",
        "print(\"\\nFrequency of tags in wikitext\\n\")\n",
        "print(df_tags.to_string(index=False))\n",
        "\n",
        "tag_counts_per_word = Counter(all_entities)\n",
        "df_tags_per_word = pd.DataFrame(tag_counts_per_word.items(), columns=[\"Word\", \"Tag\"])\n",
        "df_tags_per_word = df_tags_per_word.sort_values(\"Word\")\n",
        "\n",
        "print(\"\\nFrequency of tags in wikitext\\n\")\n",
        "print(df_tags_per_word.to_string(index=False))\n",
        "\n",
        "\"\"\"\n",
        "1) Change the training dataset CoNLL -- the one used in seminar -- so that names on nations are separated from MISC to NAT class\n",
        "Example:\n",
        "\n",
        "Spanish JJ I-NP I-MISC --> Spanish JJ I-NP I-NAT\n",
        "Brazilian JJ I-NP I-MISC --> Brazilian JJ I-NP I-NAT\n",
        "\n",
        "Use one of the foloowing strategies to detect NAT tags inside the MISC:\n",
        "\n",
        "- endings (e.g. \"*ian\", \"*an\" etc)\n",
        "- clustering of strings using word embeddings - names of nationalities will most likely found in a sigle cluster as distributioanlly similar words\n",
        "- list like https://gist.github.com/marijn/274449\n",
        "- some cobmbination of them\n",
        "\n",
        "Check consistency (by a random sample) and save a modified training datset using new laels (one new label will be added - NAT).\n",
        "Therefore, the MISC category will be split into MISC and NAT categories.\n",
        "\n",
        "2) Retrain the CoNLL dataset based on the new labelling. Use the model of your choice from seminar.\n",
        "\n",
        "3) Load text from wikitext dataset and detect all entities in texts and save list of the found entities into a text file indicating type in the format \"entity\\<TAB\\>type\".\n",
        "Count the number of the found entries and print top 20 for each category in the form of the table with frequencies \"entity\\<TAB\\>entity-type\\<TAB\\>frequency\" sorted by frequency.\n",
        "\n",
        "The output shall be two tables as shown below.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "GC-PKmLvRpuq",
        "outputId": "499e7ba0-c80e-4b61-be85-eb4b6cad1771"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity\tEntity-Type\tFrequency\n",
            "\n",
            "Frequency of tags in wikitext\n",
            "\n",
            "       Tag  Frequency\n",
            "     Japan          3\n",
            "  Japanese          2\n",
            "    Vision          2\n",
            "    Action          2\n",
            "   Europan          1\n",
            "     Raven          1\n",
            "Revolution          1\n",
            "\n",
            "Frequency of tags in wikitext\n",
            "\n",
            "      Word  Tag\n",
            "    Action    2\n",
            "   Europan    1\n",
            "     Japan    3\n",
            "  Japanese    2\n",
            "     Raven    1\n",
            "Revolution    1\n",
            "    Vision    2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1) Change the training dataset CoNLL -- the one used in seminar -- so that names on nations are separated from MISC to NAT class\\nExample:\\n\\nSpanish JJ I-NP I-MISC --> Spanish JJ I-NP I-NAT\\nBrazilian JJ I-NP I-MISC --> Brazilian JJ I-NP I-NAT\\n\\nUse one of the foloowing strategies to detect NAT tags inside the MISC:\\n\\n- endings (e.g. \"*ian\", \"*an\" etc)\\n- clustering of strings using word embeddings - names of nationalities will most likely found in a sigle cluster as distributioanlly similar words\\n- list like https://gist.github.com/marijn/274449\\n- some cobmbination of them\\n\\nCheck consistency (by a random sample) and save a modified training datset using new laels (one new label will be added - NAT).\\nTherefore, the MISC category will be split into MISC and NAT categories.\\n\\n2) Retrain the CoNLL dataset based on the new labelling. Use the model of your choice from seminar.\\n\\n3) Load text from wikitext dataset and detect all entities in texts and save list of the found entities into a text file indicating type in the format \"entity\\\\<TAB\\\\>type\".\\nCount the number of the found entries and print top 20 for each category in the form of the table with frequencies \"entity\\\\<TAB\\\\>entity-type\\\\<TAB\\\\>frequency\" sorted by frequency.\\n\\nThe output shall be two tables as shown below.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "Frequency of tags in wikitext\n",
        "\n",
        "| Tag         | Frequency    |\n",
        "|--------------|-----------|\n",
        "| LOC | 199 |\n",
        "| ORG | 99 |\n",
        "| NAT | 9 |\n",
        "| MISC | 3 |\n",
        "\n",
        "\n",
        "\n",
        "Frequency of tags in wikitext\n",
        "\n",
        "|Word | Tag | Frequency |\n",
        "|------|--------|-----------|\n",
        "|Saint-Petersbourg | LOC | 199 |\n",
        "| ORG | 99 |\n",
        "| NAT | 9 |\n",
        "| MISC | 3 |\n",
        "\n"
      ],
      "metadata": {
        "id": "C89DRaYRljw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j-e0EDgKl81E"
      }
    }
  ]
}